{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Link Prediction on a Knowledge Graph\n",
    "\n",
    "\n",
    "Link prediction is the task of predicting missing connections (or links) between two nodes in a (directed) graph. By convention, we call those two nodes the subject $s$ and the object $o$. In most of the cases, we are considering typed edges which means that the edges (ie the relations) have a label, noted as $r$. An example triplet in the context of social networks could be $\\Big(s=\"John Doe\", r=\"follows\", o=\"Karen Smith\"\\Big)$. Link prediction is as the heart of many applications with Knowledge Graphs (KG). For instance, link prediction can be used to propose to a user to connect/follow another one.\n",
    "\n",
    "In this hands-on tutorial, we propose to play with the FB15k-237 dataset.  This dataset contains textual  relations  that were extracted  from  200  million  sentences  in  the  ClueWeb12  corpus  coupled  with Freebase mention annotations and include textual links of all co-occurring entities from the KB set. The FB15k-237 dataset has 15'000 nodes and 272'000 edges. 17'000 are used for validation and 20'000 are used for testing. \n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- Sharpen your understanding of Graph Neural Networks with the R-GCN model \n",
    "- Use the high-level API of the DGL library \n",
    "- Train a link prediction task: predicting missing links in a Knowledge Graph (KG)\n",
    "\n",
    "Disclaimer: This hands-on exercise is inspired by the official DGL tutorial introducing the Relational-Graph Convolutional Network (R-GCN) available [here](https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.) Dataloading with DGL:\n",
    "\n",
    "### Load FB15k-237 dataset\n",
    "\n",
    "We can directly use the `dgl.data.knowledge_graph` module that allows to load several Knowledge Graphs, eg `FB15k-237`.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "Working with KG requires some data manipulation to extract the test graph out of the full knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graph import * \n",
    "from dgl.data.knowledge_graph import load_data\n",
    "\n",
    "# 1. load KG\n",
    "data = load_data(\"FB15k-237\")\n",
    "\n",
    "# 2. extract meta info\n",
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = torch.LongTensor(data.valid)\n",
    "test_data = torch.LongTensor(data.test)\n",
    "num_rels = data.num_rels\n",
    "\n",
    "# 3. build test graph\n",
    "test_graph, test_rel, test_norm = build_test_graph(num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))\n",
    "\n",
    "# build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.) Designing the model\n",
    "\n",
    "### High-level model design\n",
    "\n",
    "- Encode each node using a Graph Neural Network that can operates on directed node- and edges- labeled graph. An example is to use the relational GCN (R-GCN)\n",
    "- Decode using a tensor factorization method to derive scores for each candidate triplet. An example is to use the  DistMult.\n",
    "\n",
    "### Model formulation & implementation \n",
    "\n",
    "#### R-GCN\n",
    "\n",
    "We make use of the high-level API `dgl.nn.pytorch.conv` that allows to directly load GNN layers. In particular, this task is based on the R-GCN model. The R-GCN proposes to update each node as:\n",
    "\\begin{equation}\n",
    "h(v)^{(k)} = \\sigma \\Big( \\sum_{r \\in R} \\sum_{u \\in N(v)^r} \\frac{1}{c(v)^r} W_r^{(k)} h(u)^{(k-1)} + W_0^{(k)} h(v)^{(k-1)}   \\Big)\n",
    "\\end{equation}\n",
    "\n",
    "#### DistMult\n",
    "\n",
    "The probability of a triplet defined by subject, object and a relation is computed using the `DistMult` as:\n",
    "\\begin{equation}\n",
    "f(s, r, o) = e^T_s R_r e_o\n",
    "\\end{equation}\n",
    "where $e_s$ and $e_o$ are the final node embeddings of the subject and object respectively, ie $e_s = h_s^{(k=K)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from dgl.data.knowledge_graph import load_data\n",
    "from dgl.nn.pytorch import RelGraphConv\n",
    "\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "      \n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h\n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, num_rels, h_dim=500, num_bases=100,\n",
    "                 num_hidden_layers=2, dropout=0.2, reg_param=0.01):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.) Define the training and testing loop\n",
    "\n",
    "### Use classic PyTorch training loop \n",
    "- Define the model parameters (num layers, GNN dimensions)\n",
    "- Define the training parameters (optimizer, learning rate, weight decay, number of epochs)\n",
    "\n",
    "### Define positive and negative samples\n",
    "\n",
    "In order to train a link prediction system, we need to generate positive and negative triplets $(o, r, s)$ associated to a score (the triplet exists or doesn't exist).\n",
    "\n",
    "**Positive samples:** During training, the system operates on a subset of the original graph, where we simply randomly drop edges. The dropped relationships are then used to evaluate the system during inference.\n",
    "\n",
    "**Negative samples:** For each observed example we sample negative ones. We sample by randomly corrupting either the subject or the object of each positive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.graph import * \n",
    "from utils.metrics import * \n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# declare model\n",
    "model = LinkPredict(num_nodes, num_rels)\n",
    "\n",
    "# build optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-2,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = generate_sampled_graph_and_labels(train_data, num_rels, adj_list, degrees)\n",
    "\n",
    "    # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "\n",
    "    embed = model(g, node_id, edge_type, edge_norm)\n",
    "    loss = model.get_loss(embed, data, labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f}\".format(epoch, loss.item(), best_mrr))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # validation\n",
    "    if epoch % 500 == 0:\n",
    "        # perform validation on CPU because full graph is too large\n",
    "        model.cpu()\n",
    "        model.eval()\n",
    "        print(\"start eval\")\n",
    "        embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        mrr = calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n",
    "                             valid_data, test_data, hits=[1, 3, 10])\n",
    "        # save best model\n",
    "        if mrr < best_mrr:\n",
    "            if epoch >= 6000:\n",
    "                break\n",
    "        else:\n",
    "            best_mrr = mrr\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "                       'checkpoints/model_state_{}.pt'.format(str(epoch)))\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.) Testing \n",
    "\n",
    "### Metrics\n",
    "- The system is assessed using:\n",
    "    - the mean reciprocal rank ($MRR$)\n",
    "    - Hits at n ($H@n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load pre-trained network \n",
    "checkpoint = torch.load('checkpoints/model_state_500.pt')\n",
    "\n",
    "# 2. eval and compute MRR\n",
    "model.cpu() \n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
    "embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "calc_mrr(embed, model.w_relation, torch.LongTensor(train_data), valid_data, test_data, hits=[1, 3, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
